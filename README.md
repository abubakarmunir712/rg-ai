# ğŸ§  Research Genie â€“ AI Service Module
## ğŸ“˜ Overview

The **AI Service Module** is the intelligence layer of the Research Genie system.
It connects with the Backend and Scraper microservices to generate meaningful insights from research papers using LLMs (Gemini / ChatGPT).

This service refines user queries, receives relevant research papers, analyzes them, and extracts research gaps and summarized insights tailored to the userâ€™s education level.

## âš™ï¸ Core Responsibilities

- Refine user queries for efficient scraping (optional)
- Communicate with Backend and Scraper microservices
- Process user education details for customized output difficulty
- Use LLM APIs to generate:
  - Research summaries
  - Research gap analyses
  - Simplified explanations (based on user level)
- Return structured output to the Backend for UI display

## ğŸ—ï¸ Workflow Summary

- Receive query & user profile from the Backend
- (Optional) Refine the user query using NLP-based preprocessing
- Send refined query to Scraper Module
- Receive scraped papers data (metadata, abstracts, etc.)
- Process data through LLM (Gemini/ChatGPT) for:
  - Summarization
  - Gap extraction
  - Difficulty-based explanation
- Return the final structured output to the Backend

## ğŸ§  Tech Stack

| Category          | Technology                  | Purpose                                  |
| ----------------- | --------------------------- | ---------------------------------------- |
| **Language**      | Python 3.10+                | Core service logic                       |
| **Framework**     | FastAPI / Flask             | RESTful microservice API                 |
| **LLM API**       | Gemini API / OpenAI GPT API | Summarization & research gap generation  |
| **Communication** | REST API / gRPC             | Connects with Backend & Scraper services |
| **Environment**   | Docker / Docker Compose     | Containerized microservice deployment    |
| **Logging**       | Python `logging` / `loguru` | Service logs and debugging               |
| **Testing**       | Pytest                      | Unit and integration testing             |

## ğŸ“ Folder Structure

```
ai_service/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Entry point (FastAPI app)
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ ai_routes.py        # Exposes API endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ query_refiner.py    # Optional query cleaner
â”‚   â”‚   â”œâ”€â”€ llm_processor.py    # Handles LLM calls (Gemini/ChatGPT)
â”‚   â”‚   â”œâ”€â”€ communication.py    # API requests to backend/scraper
â”‚   â”‚   â”œâ”€â”€ formatter.py        # Formats final AI output (JSON)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py         # API keys, environment variables, endpoints
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ prompts.py          # LLM prompt templates
â”‚       â”œâ”€â”€ logger.py           # Logging configuration
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_llm_processor.py
â”‚   â”œâ”€â”€ test_query_refiner.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ§© API Endpoints (Example)

| Endpoint          | Method | Description                                          |
| ----------------- | ------ | ---------------------------------------------------- |
| `/refine_query`   | POST   | Refine user query before scraping                    |
| `/analyze_papers` | POST   | Process papers + user data to generate research gaps |
| `/health`         | GET    | Check service health                                 |


## ğŸš€ Setup & Run

1. Clone the Repository
```
git clone https://github.com/ResearchGenie/ai_service.git
cd ai_service
```

2. Create Virtual Environment
```
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows
```

3. Install Dependencies
```
pip install -r requirements.txt
```

4. Run Service
```
uvicorn src.main:app --reload
```
